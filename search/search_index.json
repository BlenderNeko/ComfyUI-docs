{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the ComfyUI Community Docs!","text":"<p>This is the community-maintained repository of documentation related to ComfyUI, a powerful and modular stable diffusion GUI and backend. </p> <p>The aim of this page is to get you up and running with ComfyUI, running your first gen, and providing some suggestions for the next steps to explore.</p>"},{"location":"#installation","title":"Installation","text":"<p>We won't be covering the installation of ComfyUI in detail, as the project is under active development, which tends to change the installation instructions. Instead, refer to the README on GitHub and find the sections that are relevant to your install (Linux, macOS or Windows).</p>"},{"location":"#downloading-a-model","title":"Downloading a Model","text":"<p>If you're entirely new to anything Stable Diffusion-related, the first thing you'll want to do is grab a model checkpoint that you will use to generate your images. </p> <p>Experienced Users</p> <p>If you already have files (model checkpoints, embeddings etc), there's no need to re-download those. You can keep them in the same location and just tell ComfyUI where to find them. To do this, locate the file called <code>extra_model_paths.yaml.example</code>, rename it to <code>extra_model_paths.yaml</code>, then edit the relevant lines and restart Comfy. Once that's done, skip to the next section.</p> <p>You can find a large variety of models on websites like CivitAI or HuggingFace. To start, grab a model checkpoint that you like and place it in <code>models/checkpoints</code> (create the directory if it doesn't exist yet), then re-start ComfyUI.</p>"},{"location":"#first-steps-with-comfy","title":"First Steps With Comfy","text":"<p>At this stage, you should have ComfyUI up and running in a browser tab. The default flow that's loaded is a good starting place to get familiar with. To navigate the canvas, you can either drag the canvas around, or hold Space and move your mouse. You can zoom by scrolling. </p> <p>Accidents happen</p> <p>If you mess something up, just hit <code>Load Default</code> in the menu to reset it to the inital state.</p> <p> </p> The default startup workflow of ComfyUI (open image in a new tab for better viewing) <p>Before we run our default workflow, let's make a small modification to preview the generated images without saving them:</p> <ol> <li>Right-click on the <code>Save Image</code> node, then select <code>Remove</code>.</li> <li>Double-click on an empty part of the canvas, type in <code>preview</code>, then click on the <code>PreviewImage</code> option.</li> <li>Locate the <code>IMAGE</code> output of the <code>VAE Decode</code> node and connect it to the <code>images</code> input of the <code>Preview Image</code> node you just added.</li> </ol> <p>This modification will preview your results without immediately saving them to disk. Don't worry, if you really like a particular result you can still right-click the image and choose <code>Save Image</code></p> <p>Create your first image by clicking <code>Queue Prompt</code> in the menu, or hitting Cmd+Enter or Ctrl+Enter on your keyboard, and that's it!</p>"},{"location":"#loading-other-flows","title":"Loading Other Flows","text":"<p>To make sharing easier, many Stable Diffusion interfaces, including ComfyUI, store the details of the generation flow inside the generated PNG. Many of the workflow guides you will find related to ComfyUI will also have this metadata included. To load the associated flow of a generated image, simply load the image via the <code>Load</code> button in the menu, or drag and drop it into the ComfyUI window. This will automatically parse the details and load all the relevant nodes, including their settings.</p> <p>No flow is displayed</p> <p>If you load an image but no flow is displayed, it likely means that the metadata was stripped from the file. If you know the original source of the image, try asking the author to re-upload it on a site that does not strip metadata.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>This page should have given you a good initial overview of how to get started with Comfy. Thanks to the node-based interface, you can build workflows consisting of dozens of nodes, all doing different things, allowing for some really neat image generation pipelines. </p> <p>It's also likely that you now have a lot of questions of what just happened, what each node does, and \"how do I do X thing\"-type questions. These should hopefully be answered in the rest of these docs.</p>"},{"location":"#further-support","title":"Further support","text":"<p>Have additional questions that aren't answered by the manual? Check out the ComfyUI Matrix space!</p>"},{"location":"Contributing%20Documentation/","title":"Overview page contributing documentation","text":""},{"location":"Contributing%20Documentation/Writing%20Style%20Guide/","title":"Writing Style Guide","text":"<p>below the writing style guide of the Blender manual, adapted for this project.</p>"},{"location":"Contributing%20Documentation/Writing%20Style%20Guide/#primary-goals","title":"Primary Goals","text":"<p>The main goals for this manual are as follows:</p> <code>User Focused</code> <p>The Manual is written for people with a basic understanding of using Stable Diffusion in currently available software with a basic grasp of node based programming. While some areas of machine learning and generative models are highly technical, this manual shall be kept understandable by non-technical users.</p> <code>Complete</code> <p>The manual provides detailed functional description of all nodes and features in ComfyUI. For each node or feature the manual should provide information on how to use it, and its purpose. More background information should be provided when necessary to give deeper understanding of the generative process.</p> <code>Concise</code> <p>Machine Learning is an incredibly interesting field, however, expanding into details can add unnecessary content. Keep the text concise, relevant to the topic at hand and factual.</p> <code>Maintainable</code> <p>Keep in mind that ComfyUI has frequent updates, so try to write content that will not have to be redone the moment some small change is made.</p>"},{"location":"Contributing%20Documentation/Writing%20Style%20Guide/#content-guidelines","title":"Content Guidelines","text":"<p>In order to maintain a consistent writing style within the manual, please keep this page in mind and only deviate from it when you have a good reason to do so.</p> <p>Rules of thumb:</p> <ul> <li>Spell checking is strongly recommended.</li> <li>Use American English.</li> <li>Take care about grammar, appropriate wording and use simple English.</li> <li>Keep sentences short and clear, resulting in text that is easy to read, objective and to the point.</li> <li>If you are unsure about how a feature works, ask someone else or find out who developed it and ask them.</li> </ul> <p>To be avoided:</p> <ul> <li>Avoid writing in first person perspective, about yourself or your own opinions.</li> <li>Avoid weasel words and being unnecessarily vague.</li> <li>Avoid documenting bugs. </li> <li>Avoid product placements, i.e. unnecessarily promoting specific models. Keep content neutral where possible.</li> <li>Avoid technical explanations about the mathematical/algorithmic implementation of a feature if there is a simpler way to explain it.</li> <li>Avoid repetition of large portions of text. Simply explain it once, and from then on refer to that explanation.</li> </ul>"},{"location":"Contributing%20Documentation/Writing%20Style%20Guide/#screenshot-guidelines","title":"Screenshot Guidelines","text":"<p>Individual nodes shall be captured using the Workflow SVG script, keep in mind that they will be given a width of 450px on their page. These nodes shall be of the default width and using the default dark mode theme.</p> <p>Images displaying example workflows shall clearly display the nodes in question and contain the workflow as part of their meta-data such that users can easily access the workflows. Workflows shall not be presented to users in the form of a json file, or as output images of a prompt. To attach the workflow to these screenshots the file <code>copy_pnginfo.py</code> inside the util folder can be called using <code>copy_pnginfo.py screenshot.png image_with_metadata.png output_image.png</code></p>"},{"location":"Contributing%20Documentation/templates/","title":"Templates","text":"<p>The following guide provides patterns for core and custom nodes.</p>"},{"location":"Contributing%20Documentation/templates/#node-pages","title":"Node Pages","text":"<p>Pages about nodes should always start with a brief explanation and image of the node. This is followed by two headings, inputs and outputs, with a note of absence if the node has none. At the end of the page can be an optional example(s) section:</p> <pre><code># Node Name\n![KSampler node](media/KSampler.svg){ align=right width=450 }\n\nShort description and explanation of the node\n\n## inputs\n`Lorem ipsum dolor sit amet`\n:   Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus\n    tellus non sem sollicitudin, quis rutrum leo facilisis.\n\n`Cras arcu libero`\n:   Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin\n    ut eros sed sapien ullamcorper consequat. Nunc ligula ante.\n\n    Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis.\n    Nam vulputate tincidunt fringilla.\n    Nullam dignissim ultrices urna non auctor.\n\n## outputs\nThis node has no outputs.\n\n## example\nexample usage text with workflow image\n</code></pre> <p>Examples Should be short, simple and not rely on custom nodes other than those in the nodepack to which the node on the manual page belongs to. Examples should come with an image that displays how the node fits into the example workflow, and the metadata of this image should encode the workflow depicted.</p>"},{"location":"Core%20Nodes/","title":"Overview page of ComfyUI core nodes","text":""},{"location":"Core%20Nodes/Advanced/","title":"Advanced","text":""},{"location":"Core%20Nodes/Advanced/DIffusersLoader/","title":"Diffusers Loader","text":"<p>The Diffusers Loader node can be used to load a diffusion model from diffusers.</p>"},{"location":"Core%20Nodes/Advanced/DIffusersLoader/#inputs","title":"inputs","text":"<code>model_path</code> <p>path to the diffusers model.</p>"},{"location":"Core%20Nodes/Advanced/DIffusersLoader/#outputs","title":"outputs","text":"<code>MODEL</code> <p>The model used for denoising latents.</p> <code>CLIP</code> <p>The CLIP model used for encoding text prompts.</p> <code>VAE</code> <p>The VAE model used for encoding and decoding images to and from latent space.</p>"},{"location":"Core%20Nodes/Advanced/DIffusersLoader/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Advanced/LoadCheckpointWithConfig/","title":"Load Checkpoint (With Config)","text":"<p>The Load Checkpoint (With Config) node can be used to load a diffusion model according to a supplied config file. Note that the regular load checkpoint node is able to guess the appropriate config in most of the cases.</p>"},{"location":"Core%20Nodes/Advanced/LoadCheckpointWithConfig/#inputs","title":"inputs","text":"<code>config_name</code> <p>The name of the config file.</p> <code>ckpt_name</code> <p>The name of the model to be loaded.</p>"},{"location":"Core%20Nodes/Advanced/LoadCheckpointWithConfig/#outputs","title":"outputs","text":"<code>MODEL</code> <p>The model used for denoising latents.</p> <code>CLIP</code> <p>The CLIP model used for encoding text prompts.</p> <code>VAE</code> <p>The VAE model used for encoding and decoding images to and from latent space.</p>"},{"location":"Core%20Nodes/Advanced/LoadCheckpointWithConfig/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/","title":"Conditioning","text":"<p>In ComfyUI Conditionings are used to guide the diffusion model to generate certain outputs. All conditionings start with a text prompt embedded by CLIP using a Clip Text Encode node. These conditions can then be further augmented or modified by the other nodes that can be found in this segment. </p> <p>Examples of such are guiding the process towards certain compositions using the Conditioning (Set Area), Conditioning (Set Mask), or GLIGEN Textbox Apply node.</p> <p>Or providing additional visual hints through nodes such as the Apply Style Model, Apply ControlNet or unCLIP Conditioning node. A full list of relevant nodes can be found in the sidebar.</p>"},{"location":"Core%20Nodes/Conditioning/ApplyControlNet/","title":"Apply ControlNet","text":"<p>The Apply ControlNet node can be used to provide further visual guidance to a diffusion model. Unlike unCLIP embeddings, controlnets and T2I adaptors work on any model. By chaining together multiple nodes it is possible to guide the diffusion model using multiple controlNets or T2I adaptors. This can be useful to e.g. hint at the diffusion model where the edges in the final image should be by providing an image containing edge detections along with a controlNet trained on edge detection images to this node.</p> <p>Info</p> <p>To use the T2IAdaptor style model see the Apply Style Model node instead</p>"},{"location":"Core%20Nodes/Conditioning/ApplyControlNet/#inputs","title":"inputs","text":"<code>conditioning</code> <p>A conditioning.</p> <code>control_net</code> <p>A controlNet or T2IAdaptor, trained to guide the diffusion model using specific image data.</p> <code>image</code> <p>The image used as a visual guide for the diffusion model.</p>"},{"location":"Core%20Nodes/Conditioning/ApplyControlNet/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A Conditioning containing the control_net and visual guide.</p>"},{"location":"Core%20Nodes/Conditioning/ApplyControlNet/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/ApplyStyleModel/","title":"Apply Style Model","text":"<p>The Apply Style Model node can be used to provide further visual guidance to a diffusion model specifically pertaining to the style of the generated images. This node takes the T2I Style adaptor model and an embedding from a CLIP vision model to guide a diffusion model towards the style of the image embedded by CLIP vision.</p>"},{"location":"Core%20Nodes/Conditioning/ApplyStyleModel/#inputs","title":"inputs","text":"<code>conditioning</code> <p>A conditioning.</p> <code>style_model</code> <p>A T2I style adaptor.</p> <code>CLIP_vision_output</code> <p>The image containing the desired style, encoded by a CLIP vision model.</p>"},{"location":"Core%20Nodes/Conditioning/ApplyStyleModel/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A Conditioning containing the T2I style adaptor and visual guide towards the desired style.</p>"},{"location":"Core%20Nodes/Conditioning/ApplyStyleModel/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/CLIPSetLastLayer/","title":"CLIP Set Last Layer","text":"<p>The CLIP Set Last Layer node can be used to set the CLIP output layer from which to take the text embeddings. Encoding text into an embedding happens by the text being transformed by various layers in the CLIP model. Although traditionally diffusion models are conditioned on the output of the last layer in CLIP, some diffusion models have been conditioned on earlier layers and might not work as well when using the output of the last layer.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPSetLastLayer/#inputs","title":"inputs","text":"<code>clip</code> <p>The CLIP model used for encoding the text.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPSetLastLayer/#outputs","title":"outputs","text":"<code>CLIP</code> <p>The CLIP model with the newly set output layer.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPSetLastLayer/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/CLIPTextEncode/","title":"CLIP Text Encode (Prompt)","text":"<p>The CLIP Text Encode node can be used to encode a text prompt using a CLIP model into an embedding that can be used to guide the diffusion model towards generating specific images. For a complete guide of all text prompt related features in ComfyUI see this page.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPTextEncode/#inputs","title":"inputs","text":"<code>clip</code> <p>The CLIP model used for encoding the text.</p> <code>text</code> <p>The text to be encoded.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPTextEncode/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A Conditioning containing the embedded text used to guide the diffusion model.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPTextEncode/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/CLIPVisionEncode/","title":"CLIP Vision Encode","text":"<p>The CLIP Vision Encode node can be used to encode an image using a CLIP vision model into an embedding that can be used to guide unCLIP diffusion models or as input to style models.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPVisionEncode/#inputs","title":"inputs","text":"<code>clip_vision</code> <p>The CLIP vision model used for encoding the image.</p> <code>image</code> <p>The image to be encoded.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPVisionEncode/#outputs","title":"outputs","text":"<code>CLIP_VISION_OUTPUT</code> <p>The encoded image.</p>"},{"location":"Core%20Nodes/Conditioning/CLIPVisionEncode/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningAverage/","title":"Conditioning (Average)","text":"<p>The Conditioning (Average) node can be used to interpolate between two text embeddings according to a strength factor set in <code>conditioning_to_strength</code>.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningAverage/#inputs","title":"inputs","text":"<code>conditioning_to</code> <p>The conditioning with the text embeddings at <code>conditioning_to_strength</code> of 1.</p> <code>conditioning_from</code> <p>The conditioning with the text embeddings at <code>conditioning_to_strength</code> of 0.</p> <code>conditioning_to_strength</code> <p>The factor by which to mix <code>conditioning_to</code> into <code>conditioning_from</code>.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningAverage/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A new conditioning with the text embeddings mixed based on <code>conditioning_to_strength</code>.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningAverage/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningCombine/","title":"Conditioning (Combine)","text":"<p>The Conditioning (Combine) node can be used to combine multiple conditionings by averaging the predicted noise of the diffusion model. Note that this is different from the Conditioning (Average) node. Here outputs of the diffusion model conditioned on different conditionings (i.e. all parts that make up the conditioning) are averaged out, while the Conditioning (Average) node interpolates the text embeddings that are stored inside the conditioning.</p> <p>Tip</p> <p>Even though Conditioning Combine does not have a factor input to determine how to interpolate the two resulting noise predictions, the Conditioning (Set Area) node can be used to weight the individual conditionings before combining them.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningCombine/#inputs","title":"inputs","text":"<code>conditioning_1</code> <p>The First conditioning.</p> <code>conditioning_2</code> <p>The second conditioning.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningCombine/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A new conditioning containing both inputs, later to be averaged by the sampler.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningCombine/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningSetArea/","title":"Conditioning (Set Area)","text":"<p>The Conditioning (Set Area) node can be used to limit a conditioning to a specified area of the image. Together with the Conditioning (Combine) node this can be used to add more control over the composition of the final image.</p> <p>Info</p> <p>The origin of the coordinate system in ComfyUI is at the top left corner.</p> <p>Info</p> <p><code>strength</code> is normalized before mixing multiple noise predictions from the diffusion model. </p>"},{"location":"Core%20Nodes/Conditioning/ConditioningSetArea/#inputs","title":"inputs","text":"<code>conditioning</code> <p>The conditioning that will be limited to an area.</p> <code>width</code> <p>The width of the area.</p> <code>height</code> <p>The height of the area.</p> <code>x</code> <p>The x coordinate of the area.</p> <code>y</code> <p>The y coordinate of the area.</p> <code>strength</code> <p>The weight of the area to be used when mixing multiple overlapping conditionings.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningSetArea/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A new conditioning limited to the specified area.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningSetArea/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningSetMask/","title":"Conditioning (Set Mask)","text":"<p>The Conditioning (Set Mask) node can be used to limit a conditioning to a specified mask. Together with the Conditioning (Combine) node this can be used to add more control over the composition of the final image. </p> <p>Info</p> <p><code>strength</code> is normalized before mixing multiple noise predictions from the diffusion model. </p>"},{"location":"Core%20Nodes/Conditioning/ConditioningSetMask/#inputs","title":"inputs","text":"<code>conditioning</code> <p>The conditioning that will be limited to a mask.</p> <code>mask</code> <p>The mask to constrain the conditioning to.</p> <code>strength</code> <p>The weight of the masked area to be used when mixing multiple overlapping conditionings.</p> <code>set_cond_area</code> <p>Whether to denoise the whole area, or limit it to the bounding box of the mask.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningSetMask/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A new conditioning limited to the specified mask.</p>"},{"location":"Core%20Nodes/Conditioning/ConditioningSetMask/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/GLIGENTextboxApply/","title":"GLIGEN Textbox Apply","text":"<p>The GLIGEN Textbox Apply node can be used to provide further spatial guidance to a diffusion model, guiding it to generate the specified parts of the prompt in a specific region of the image. Although the text input will accept any text, GLIGEN works best if the input to it is an object that is part of the text prompt.</p> <p>Info</p> <p>The origin of the coordinate system in ComfyUI is at the top left corner.</p>"},{"location":"Core%20Nodes/Conditioning/GLIGENTextboxApply/#inputs","title":"inputs","text":"<code>conditioning_to</code> <p>A conditioning.</p> <code>clip</code> <p>A CLIP model.</p> <code>gligen_textbox_model</code> <p>A GLIGEN model.</p> <code>text</code> <p>The text to associate the spatial information to.</p> <code>width</code> <p>The width of the area.</p> <code>height</code> <p>The height of the area.</p> <code>x</code> <p>The x coordinate of the area.</p> <code>y</code> <p>The y coordinate of the area.</p>"},{"location":"Core%20Nodes/Conditioning/GLIGENTextboxApply/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A Conditioning containing GLIGEN and the spatial guidance.</p>"},{"location":"Core%20Nodes/Conditioning/GLIGENTextboxApply/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Conditioning/unCLIPConditioning/","title":"unCLIP Conditioning","text":"<p>The unCLIP Conditioning node can be used to provide unCLIP models with additional visual guidance through images encoded by a CLIP vision model. This node can be chained to provide multiple images as guidance.</p> <p>Warning</p> <p>Not all diffusion models are compatible with unCLIP conditioning. This node specifically requires a a diffusion model that was made with unCLIP in mind.</p>"},{"location":"Core%20Nodes/Conditioning/unCLIPConditioning/#inputs","title":"inputs","text":"<code>conditioning</code> <p>The conditioning.</p> <code>clip_vision_output</code> <p>An image encoded by a CLIP VISION model.</p> <code>strength</code> <p>How strongly the unCLIP diffusion model should be guided by the image</p> <code>noise_augmentation</code> <p>Noise_augmentation can be used to guide the unCLIP diffusion model to random places in the neighborhood of the original CLIP vision embeddings, providing additional variations of the generated image closely related to the encoded image.</p>"},{"location":"Core%20Nodes/Conditioning/unCLIPConditioning/#outputs","title":"outputs","text":"<code>CONDITIONING</code> <p>A Conditioning containing additional visual guidance for unCLIP models.</p>"},{"location":"Core%20Nodes/Conditioning/unCLIPConditioning/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Experimental/","title":"Experimental","text":"<p>Experimental contains experimental nodes that might not be fully polished yet.</p>"},{"location":"Core%20Nodes/Experimental/LoadLatent/","title":"Load Latent","text":"<p>The Load Latent node can be used to to load latents that were saved with the Save Latent node.</p>"},{"location":"Core%20Nodes/Experimental/LoadLatent/#inputs","title":"inputs","text":"<code>latent</code> <p>The name of the latent to load.</p>"},{"location":"Core%20Nodes/Experimental/LoadLatent/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The latent image.</p>"},{"location":"Core%20Nodes/Experimental/LoadLatent/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Experimental/SaveLatent/","title":"Save Latent","text":"<p>The Save Latent node can be used to to save latents for later use. These can then be loaded again using the Load Latent node.</p>"},{"location":"Core%20Nodes/Experimental/SaveLatent/#inputs","title":"inputs","text":"<code>samples</code> <p>The latents to be saved.</p> <code>filename_prefix</code> <p>a prefix for the file name.</p>"},{"location":"Core%20Nodes/Experimental/SaveLatent/#outputs","title":"outputs","text":"<p>This node has no outputs.</p>"},{"location":"Core%20Nodes/Experimental/SaveLatent/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Experimental/TomePatchModel/","title":"Tome Patch Model","text":"<p>The Tome Patch Model node can be used to apply Tome optimizations to the diffusion model. Tome (TOken MErging) tries to find a way to merge prompt tokens in such a way that the effect on the final image are minimal. This results in faster generation times and a reduction in required VRAM at the cost of a potential reduction in quality. This tradeoff can be controlled by the <code>ratio</code> setting, where higher values result in more tokens being merged</p>"},{"location":"Core%20Nodes/Experimental/TomePatchModel/#inputs","title":"inputs","text":"<code>model</code> <p>The diffusion model for tome to be applied to.</p> <code>ratio</code> <p>The treshold to determine when to merge tokens.</p>"},{"location":"Core%20Nodes/Experimental/TomePatchModel/#outputs","title":"outputs","text":"<code>MODEL</code> <p>The diffusion model optimized by tome.</p>"},{"location":"Core%20Nodes/Experimental/TomePatchModel/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Experimental/VAEDecodeTiled/","title":"VAE Decode (Tiled)","text":"<p>The VAE Decode (Tiled) node can be used to decode latent space images back into pixel space images, using the provided VAE. This node decodes latents in tiles allowing it to decode larger latent images than the regular VAE Decode node.</p> <p>Info</p> <p>When the regular VAE Decode node fails due to insufficient VRAM, comfy will automatically retry using the tiled implementation</p>"},{"location":"Core%20Nodes/Experimental/VAEDecodeTiled/#inputs","title":"inputs","text":"<code>samples</code> <p>The latent images to be decoded.</p> <code>vae</code> <p>The VAE to use for decoding the latent images.</p>"},{"location":"Core%20Nodes/Experimental/VAEDecodeTiled/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The decoded images.</p>"},{"location":"Core%20Nodes/Experimental/VAEDecodeTiled/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Experimental/VAEEncodeTiled/","title":"VAE Encode (Tiled)","text":"<p>The VAE Encode node can be used to encode pixel space images into latent space images, using the provided VAE. This node encodes images in tiles allowing it to encode larger images than the regular VAE Encode node.</p> <p>Info</p> <p>When the regular VAE Encode node fails due to insufficient VRAM, comfy will automatically retry using the tiled implementation</p>"},{"location":"Core%20Nodes/Experimental/VAEEncodeTiled/#inputs","title":"inputs","text":"<code>pixels</code> <p>The pixel space images to be encoded.</p> <code>vae</code> <p>The VAE to use for encoding the pixel images.</p>"},{"location":"Core%20Nodes/Experimental/VAEEncodeTiled/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The encoded latent images.</p>"},{"location":"Core%20Nodes/Experimental/VAEEncodeTiled/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/","title":"Image","text":"<p>ComfyUI provides a variety of nodes to manipulate pixel images. These nodes can be used to load images for img2img workflows, save results, or e.g. upscale images for a highres workflow.</p>"},{"location":"Core%20Nodes/Image/InvertImage/","title":"Invert Image","text":"<p>The Invert Image node can be used to to invert the colors of an image.</p>"},{"location":"Core%20Nodes/Image/InvertImage/#inputs","title":"inputs","text":"<code>image</code> <p>The pixel image to be inverted.</p>"},{"location":"Core%20Nodes/Image/InvertImage/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The inverted pixel image.</p>"},{"location":"Core%20Nodes/Image/InvertImage/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/LoadImage/","title":"Load Image","text":"<p>The Load Image node can be used to to load an image. Images can be uploaded by starting the file dialog or by dropping an image onto the node. Once the image has been uploaded they can be selected inside the node.</p> <p>Info</p> <p>by default images will be uploaded to the input folder of ComfyUI</p>"},{"location":"Core%20Nodes/Image/LoadImage/#inputs","title":"inputs","text":"<code>image</code> <p>The name of the image to use.</p>"},{"location":"Core%20Nodes/Image/LoadImage/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The pixel image.</p> <code>MASK</code> <p>The alpha channel of the image.</p>"},{"location":"Core%20Nodes/Image/LoadImage/#example","title":"example","text":"<p>In order to perform image to image generations you have to load the image with the load image node. In the example below an image is loaded using the load image node, and is then encoded to latent space with a VAE encode node, letting us perform image to image tasks.</p> <p>(TODO: provide different example using mask)</p>"},{"location":"Core%20Nodes/Image/PadImageForOutpainting/","title":"Pad Image for Outpainting","text":"<p>The Pad Image for Outpainting node can be used to to add padding to an image for outpainting. This image can then be given to an inpaint diffusion model via the VAE Encode for Inpainting.</p>"},{"location":"Core%20Nodes/Image/PadImageForOutpainting/#inputs","title":"inputs","text":"<code>image</code> <p>The image to be padded.</p> <code>left</code> <p>amount to pad left of the image.</p> <code>top</code> <p>amount to pad above the image.</p> <code>right</code> <p>amount to pad right of the image.</p> <code>bottom</code> <p>amount to pad below the image.</p> <code>feathering</code> <p>How much to feather the borders of the original image.</p>"},{"location":"Core%20Nodes/Image/PadImageForOutpainting/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The padded pixel image.</p> <code>MASK</code> <p>The mask indicating to the sampler where to outpaint.</p>"},{"location":"Core%20Nodes/Image/PadImageForOutpainting/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/PreviewImage/","title":"Preview Image","text":"<p>The Preview Image node can be used to preview images inside the node graph.</p>"},{"location":"Core%20Nodes/Image/PreviewImage/#inputs","title":"inputs","text":"<code>image</code> <p>The pixel image to preview.</p>"},{"location":"Core%20Nodes/Image/PreviewImage/#outputs","title":"outputs","text":"<p>This node has no outputs.</p>"},{"location":"Core%20Nodes/Image/PreviewImage/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/SaveImage/","title":"Save Image","text":"<p>The Save Image node can be used to save images. To simply preview an image inside the node graph use the Preview Image node. It can be hard to keep track of all the images that you generate. To help with organizing your images you can pass specially formatted strings to an output node with a <code>file_prefix</code> widget. For more information about how to format your string see this page.</p>"},{"location":"Core%20Nodes/Image/SaveImage/#inputs","title":"inputs","text":"<code>image</code> <p>The pixel image to preview.</p> <code>filename_prefix</code> <p>A prefix to put into the filename.</p>"},{"location":"Core%20Nodes/Image/SaveImage/#outputs","title":"outputs","text":"<p>This node has no outputs.</p>"},{"location":"Core%20Nodes/Image/SaveImage/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageBlend/","title":"Image Blend","text":"<p>The Image Blend node can be used to blend two images together.</p> <p>Info</p> <p>If the dimensions of the second image do not match those of the first it is rescaled and center-cropped to maintain its aspect ratio</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageBlend/#inputs","title":"inputs","text":"<code>image1</code> <p>A pixel image.</p> <code>image2</code> <p>A second pixel image.</p> <code>blend_factor</code> <p>The opacity of the second image.</p> <code>blend_mode</code> <p>How to blend the images.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageBlend/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The blended pixel image.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageBlend/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageBlur/","title":"Image Blur","text":"<p>The Image Blend node can be used to apply a gaussian blur to an image.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageBlur/#inputs","title":"inputs","text":"<code>image</code> <p>The pixel image to be blurred.</p> <code>blur_radius</code> <p>The radius of the gaussian.</p> <code>sigma</code> <p>The sigma of the gaussian, the smaller sigma is the more the kernel in concentrated on the center pixel.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageBlur/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The blurred pixel image.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageBlur/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageQuantize/","title":"Image Quantize","text":"<p>The Image Quantize node can be used to quantize an image, reducing the number of colors in the image.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageQuantize/#inputs","title":"inputs","text":"<code>image</code> <p>The pixel image to be quantized.</p> <code>colors</code> <p>The number of colors in the quantized image.</p> <code>dither</code> <p>Wether to use dithering to make the quantized image look more smooth, or not.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageQuantize/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The quantized pixel image.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageQuantize/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageSharpen/","title":"Image Sharpen","text":"<p>The Image Sharpen node can be used to apply a Laplacian sharpening filter to an image.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageSharpen/#inputs","title":"inputs","text":"<code>image</code> <p>The pixel image to be sharpened.</p> <code>sharpen_radius</code> <p>The radius of the sharpening kernel.</p> <code>sigma</code> <p>The sigma of the gaussian, the smaller sigma is the more the kernel in concentrated on the center pixel.</p> <code>alpha</code> <p>The strength of the sharpening kernel.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageSharpen/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The sharpened pixel image.</p>"},{"location":"Core%20Nodes/Image/postprocessing/ImageSharpen/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/upscaling/UpscaleImage/","title":"Upscale Image","text":"<p>The Upscale Image node can be used to resize pixel images. To upscale images using AI see the Upscale Image Using Model node.</p>"},{"location":"Core%20Nodes/Image/upscaling/UpscaleImage/#inputs","title":"inputs","text":"<code>image</code> <p>The pixel images to be upscaled.</p> <code>upscale_method</code> <p>The method used for resizing.</p> <code>Width</code> <p>The target width in pixels.</p> <code>height</code> <p>The target height in pixels.</p> <code>crop</code> <p>Wether or not to center-crop the image to maintain the aspect ratio of the original latent images.</p>"},{"location":"Core%20Nodes/Image/upscaling/UpscaleImage/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The resized images.</p>"},{"location":"Core%20Nodes/Image/upscaling/UpscaleImage/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Image/upscaling/UpscaleImageUsingModel/","title":"Upscale Image (using Model)","text":"<p>The Upscale Image (using Model) node can be used to upscale pixel images using a model loaded with the Load Upscale Model node.</p>"},{"location":"Core%20Nodes/Image/upscaling/UpscaleImageUsingModel/#inputs","title":"inputs","text":"<code>upscale_model</code> <p>The model used for upscaling.</p> <code>image</code> <p>The pixel images to be upscaled.</p>"},{"location":"Core%20Nodes/Image/upscaling/UpscaleImageUsingModel/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The upscaled images.</p>"},{"location":"Core%20Nodes/Image/upscaling/UpscaleImageUsingModel/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/","title":"Latent","text":"<p>Latent diffusion models such as Stable Diffusion do not operate in pixel space, but denoise in latent space instead. These nodes provide ways to switch between pixel and latent space using encoders and decoders, and provide a variety of ways to manipulate latent images.</p>"},{"location":"Core%20Nodes/Latent/EmptyLatentImage/","title":"Empty Latent Image","text":"<p>The Empty Latent Image node can be used to create a new set of empty latent images. These latents can then be used inside e.g. a text2image workflow by noising and denoising them with a sampler node.</p>"},{"location":"Core%20Nodes/Latent/EmptyLatentImage/#inputs","title":"inputs","text":"<code>width</code> <p>The width of the latent images in pixels.</p> <code>height</code> <p>The height of the latent images in pixels.</p> <code>batch_size</code> <p>The number of latent images.</p>"},{"location":"Core%20Nodes/Latent/EmptyLatentImage/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The empty latent images.</p>"},{"location":"Core%20Nodes/Latent/EmptyLatentImage/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/LatentComposite/","title":"Latent Composite","text":"<p>The Latent Composite node can be used to paste one latent into another.</p> <p>Info</p> <p>The origin of the coordinate system in ComfyUI is at the top left corner.</p>"},{"location":"Core%20Nodes/Latent/LatentComposite/#inputs","title":"inputs","text":"<code>samples_to</code> <p>The latents to be pasted in.</p> <code>samples_from</code> <p>The latents that are to be pasted.</p> <code>x</code> <p>The x coordinate of the pasted latent in pixels.</p> <code>y</code> <p>The y coordinate of the pasted latent in pixels.</p> <code>feather</code> <p>Feathering for the latents that are to be pasted.</p>"},{"location":"Core%20Nodes/Latent/LatentComposite/#outputs","title":"outputs","text":"<code>LATENT</code> <p>A new latent composite containing the <code>samples_from</code> pasted into <code>samples_to</code>.</p>"},{"location":"Core%20Nodes/Latent/LatentComposite/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/LatentCompositeMasked/","title":"Latent Composite Masked","text":"<p>The Latent Composite Masked node can be used to paste a masked latent into another.</p> <p>Info</p> <p>The origin of the coordinate system in ComfyUI is at the top left corner.</p>"},{"location":"Core%20Nodes/Latent/LatentCompositeMasked/#inputs","title":"inputs","text":"<code>destination</code> <p>The latents to be pasted in.</p> <code>source</code> <p>The latents that are to be pasted.</p> <code>mask</code> <p>The mask for the source latents that are to be pasted.</p> <code>x</code> <p>The x coordinate of the pasted latent in pixels.</p> <code>y</code> <p>The y coordinate of the pasted latent in pixels.</p>"},{"location":"Core%20Nodes/Latent/LatentCompositeMasked/#outputs","title":"outputs","text":"<code>LATENT</code> <p>A new latent composite containing the <code>source</code> latents pasted into the <code>destination</code> latents.</p>"},{"location":"Core%20Nodes/Latent/LatentCompositeMasked/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/UpscaleLatent/","title":"Upscale Latent","text":"<p>The Upscale Latent node can be used to resize latent images. </p> <p>Warning</p> <p>Resizing latent images is not the same as resizing pixel images. Naively resizing the latents rather than the pixels results in more artifacts.</p>"},{"location":"Core%20Nodes/Latent/UpscaleLatent/#inputs","title":"inputs","text":"<code>samples</code> <p>The latent images to be upscaled.</p> <code>upscale_method</code> <p>The method used for resizing.</p> <code>Width</code> <p>The target width in pixels.</p> <code>height</code> <p>The target height in pixels.</p> <code>crop</code> <p>Wether or not to center-crop the image to maintain the aspect ratio of the original latent images.</p>"},{"location":"Core%20Nodes/Latent/UpscaleLatent/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The resized latents.</p>"},{"location":"Core%20Nodes/Latent/UpscaleLatent/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/VAEDecode/","title":"VAE Decode","text":"<p>The VAE Decode node can be used to decode latent space images back into pixel space images, using the provided VAE.</p>"},{"location":"Core%20Nodes/Latent/VAEDecode/#inputs","title":"inputs","text":"<code>samples</code> <p>The latent images to be decoded.</p> <code>vae</code> <p>The VAE to use for decoding the latent images.</p>"},{"location":"Core%20Nodes/Latent/VAEDecode/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The decoded images.</p>"},{"location":"Core%20Nodes/Latent/VAEDecode/#example","title":"example","text":"<p>TODO: SD 1.5 to XL example</p>"},{"location":"Core%20Nodes/Latent/VAEEncode/","title":"VAE Encode","text":"<p>The VAE Encode node can be used to encode pixel space images into latent space images, using the provided VAE.</p>"},{"location":"Core%20Nodes/Latent/VAEEncode/#inputs","title":"inputs","text":"<code>pixels</code> <p>The pixel space images to be encoded.</p> <code>vae</code> <p>The VAE to use for encoding the pixel images.</p>"},{"location":"Core%20Nodes/Latent/VAEEncode/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The encoded latent images.</p>"},{"location":"Core%20Nodes/Latent/VAEEncode/#example","title":"example","text":"<p>In order to use images in e.g. image to image tasks, they first need to be encoded into latent space. In the below example the VAE encode node is used to convert a pixel image into a latent image so that we can re-and de-noise this image into something new.</p>"},{"location":"Core%20Nodes/Latent/batch/LatentFromBatch/","title":"Latent From Batch","text":"<p>The Latent From Batch node can be used to pick a slice from a batch of latents. This is useful when a specific latent image or images inside the batch need to be isolated in the workflow.</p>"},{"location":"Core%20Nodes/Latent/batch/LatentFromBatch/#inputs","title":"inputs","text":"<code>samples</code> <p>The batch of latent images to pick a slice from.</p> <code>batch_index</code> <p>The index of the first latent image to pick.</p> <code>length</code> <p>How many latent images to take.</p>"},{"location":"Core%20Nodes/Latent/batch/LatentFromBatch/#outputs","title":"outputs","text":"<code>LATENT</code> <p>A new batch of latent images only containing the slice that was picked.</p>"},{"location":"Core%20Nodes/Latent/batch/LatentFromBatch/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/batch/RebatchLatents/","title":"Rebatch Latents","text":"<p>The Rebatch latents node can be used to split or combine batches of latent images. When this results in multiple batches the node will output a list of batches instead of a single batch. This is useful e.g. to split batches up when the batch size is too big for all of them to fit inside VRAM, as ComfyUI will execute nodes for every batch in the list, rather than all at once. It can also be used merge lists of batches back together into a single batch.</p> <p>Info</p> <p>The output of this node is a list, read this page for more info about lists in comfy. TODO:figure out when and where to explain this.</p>"},{"location":"Core%20Nodes/Latent/batch/RebatchLatents/#inputs","title":"inputs","text":"<code>samples</code> <p>The latent images that are to be rebatched.</p> <code>batch_size</code> <p>The new batch size.</p>"},{"location":"Core%20Nodes/Latent/batch/RebatchLatents/#outputs","title":"outputs","text":"<code>LATENT</code> <p>A list of latents where each batch is no larger than <code>batch_size</code>.</p>"},{"location":"Core%20Nodes/Latent/batch/RebatchLatents/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/batch/RepeatLatentBatch/","title":"Repeat Latent Batch","text":"<p>The Repeat Latent Batch node can be used to repeat a batch of latent images. This can e.g. be used to create multiple variations of an image in an image to image workflow.</p>"},{"location":"Core%20Nodes/Latent/batch/RepeatLatentBatch/#inputs","title":"inputs","text":"<code>samples</code> <p>The batch of latent images that are to be repeated.</p> <code>amount</code> <p>The number of repeats.</p>"},{"location":"Core%20Nodes/Latent/batch/RepeatLatentBatch/#outputs","title":"outputs","text":"<code>LATENT</code> <p>A new batch of latent images, repeated <code>amount</code> times.</p>"},{"location":"Core%20Nodes/Latent/batch/RepeatLatentBatch/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/inpaint/SetLatentNoiseMask/","title":"Set Latent Noise Mask","text":"<p>The Set Latent Noise Mask node can be used to add a mask to the latent images for inpainting. When the noise mask is set a sampler node will only operate on the masked area. If a single mask is provided, all the latents in the batch will use this mask.</p>"},{"location":"Core%20Nodes/Latent/inpaint/SetLatentNoiseMask/#inputs","title":"inputs","text":"<code>samples</code> <p>The latent images to be masked for inpainting.</p> <code>mask</code> <p>The mask indicating where to inpaint.</p>"},{"location":"Core%20Nodes/Latent/inpaint/SetLatentNoiseMask/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The masked latents.</p>"},{"location":"Core%20Nodes/Latent/inpaint/SetLatentNoiseMask/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/inpaint/VAEEncodeForInpainting/","title":"VAE Encode (for Inpainting)","text":"<p>The VAE Encode For Inpainting node can be used to encode pixel space images into latent space images, using the provided VAE. It also takes a mask for inpainting, indicating to a sampler node which parts of the image should be denoised. The area of the mask can be increased using <code>grow_mask_by</code> to provide the inpainting process with some additional padding to work with.</p> <p>Info</p> <p>This node is specifically meant to be used for diffusion models trained for inpainting and will make sure the pixels underneath the mask are set to gray (0.5,0.5,0.5) before encoding.</p>"},{"location":"Core%20Nodes/Latent/inpaint/VAEEncodeForInpainting/#inputs","title":"inputs","text":"<code>pixels</code> <p>The pixel space images to be encoded.</p> <code>vae</code> <p>The VAE to use for encoding the pixel images.</p> <code>mask</code> <p>The mask indicating where to inpaint.</p> <code>grow_mask_by</code> <p>How much to increase the area of the given mask.</p>"},{"location":"Core%20Nodes/Latent/inpaint/VAEEncodeForInpainting/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The masked and encoded latent images.</p>"},{"location":"Core%20Nodes/Latent/inpaint/VAEEncodeForInpainting/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/transform/CropLatent/","title":"Crop Latent","text":"<p>The Crop latent node can be used to crop latents to a new shape.</p> <p>Info</p> <p>The origin of the coordinate system in ComfyUI is at the top left corner.</p>"},{"location":"Core%20Nodes/Latent/transform/CropLatent/#inputs","title":"inputs","text":"<code>samples</code> <p>The latents that are to be cropped.</p> <code>width</code> <p>The width of the area in pixels.</p> <code>height</code> <p>The height of the area in pixels.</p> <code>x</code> <p>The x coordinate of the area in pixels.</p> <code>y</code> <p>The y coordinate of the area in pixels.</p>"},{"location":"Core%20Nodes/Latent/transform/CropLatent/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The cropped latents.</p>"},{"location":"Core%20Nodes/Latent/transform/CropLatent/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/transform/FlipLatent/","title":"Flip Latent","text":"<p>The Flip latent node can be used to flip latents horizontally or vertically.</p>"},{"location":"Core%20Nodes/Latent/transform/FlipLatent/#inputs","title":"inputs","text":"<code>samples</code> <p>The latents that are to be flipped.</p> <code>flip_method</code> <p>Wether to flip the latents horizontally or vertically.</p>"},{"location":"Core%20Nodes/Latent/transform/FlipLatent/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The flipped latents.</p>"},{"location":"Core%20Nodes/Latent/transform/FlipLatent/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Latent/transform/RotateLatent/","title":"Rotate Latent","text":"<p>The Rotate Latent node can be used to rotate latent images clockwise in increments of 90 degrees.</p>"},{"location":"Core%20Nodes/Latent/transform/RotateLatent/#inputs","title":"inputs","text":"<code>samples</code> <p>The latent images to be rotated.</p> <code>rotation</code> <p>Clockwise rotation.</p>"},{"location":"Core%20Nodes/Latent/transform/RotateLatent/#outputs","title":"outputs","text":"<code>LATENT</code> <p>The rotated latents.</p>"},{"location":"Core%20Nodes/Latent/transform/RotateLatent/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/","title":"Loaders","text":"<p>The loaders in this segment can be used to load a variety of models used in various workflows. A full list of all of the loaders can be found in the sidebar.</p>"},{"location":"Core%20Nodes/Loaders/GLIGENLoader/","title":"GLIGEN Loader","text":"<p>The GLIGEN Loader node can be used to load a specific GLIGEN model. GLIGEN models are used to associate spatial information to parts of a text prompt, guiding the diffusion model to generate images adhering to compositions specified by GLIGEN.</p>"},{"location":"Core%20Nodes/Loaders/GLIGENLoader/#inputs","title":"inputs","text":"<code>gligen_name</code> <p>The name of the GLIGEN model.</p>"},{"location":"Core%20Nodes/Loaders/GLIGENLoader/#outputs","title":"outputs","text":"<code>GLIGEN</code> <p>The GLIGEN model used to encode spatial information to parts of the text prompt.</p>"},{"location":"Core%20Nodes/Loaders/GLIGENLoader/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/HypernetworkLoader/","title":"Hypernetwork Loader","text":"<p>The Hypernetwork Loader node can be used to load a hypernetwork. similar to LoRAs, they are used to modify the diffusion model, to alter the way in which latents are denoised. Typical use-cases include adding to the model the ability to generate in certain styles, or better generate certain subjects or actions. One can even chain multiple hypernetworks together to further modify the model.</p> <p>Tip</p> <p>Hypernetwork strength values can be set to negative values. At times this can result in interesting effects.</p>"},{"location":"Core%20Nodes/Loaders/HypernetworkLoader/#inputs","title":"inputs","text":"<code>model</code> <p>A diffusion model.</p> <code>hypernetwork_name</code> <p>The name of the hypernetwork.</p> <code>strength</code> <p>How strongly to modify the diffusion model. This value can be negative.</p>"},{"location":"Core%20Nodes/Loaders/HypernetworkLoader/#outputs","title":"outputs","text":"<code>MODEL</code> <p>The modified diffusion model.</p>"},{"location":"Core%20Nodes/Loaders/HypernetworkLoader/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/LoadCLIP/","title":"Load CLIP","text":"<p>The Load CLIP node can be used to load a specific CLIP model, CLIP models are used to encode text prompts that guide the diffusion process.</p> <p>Warning</p> <p>Conditional diffusion models are trained using a specific CLIP model, using a different model than the one which it was trained with is unlikely to result in good images. The Load Checkpoint node automatically loads the correct CLIP model.</p>"},{"location":"Core%20Nodes/Loaders/LoadCLIP/#inputs","title":"inputs","text":"<code>clip_name</code> <p>The name of the CLIP model.</p>"},{"location":"Core%20Nodes/Loaders/LoadCLIP/#outputs","title":"outputs","text":"<code>CLIP</code> <p>The CLIP model used for encoding text prompts.</p>"},{"location":"Core%20Nodes/Loaders/LoadCLIP/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/LoadCLIPVision/","title":"Load CLIP Vision","text":"<p>The Load CLIP Vision node can be used to load a specific CLIP vision model, similar to how CLIP models are used to encode text prompts, CLIP vision models are used to encode images.</p>"},{"location":"Core%20Nodes/Loaders/LoadCLIPVision/#inputs","title":"inputs","text":"<code>clip_name</code> <p>The name of the CLIP vision model.</p>"},{"location":"Core%20Nodes/Loaders/LoadCLIPVision/#outputs","title":"outputs","text":"<code>CLIP_VISION</code> <p>The CLIP vision model used for encoding image prompts.</p>"},{"location":"Core%20Nodes/Loaders/LoadCLIPVision/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/LoadCheckpoint/","title":"Load Checkpoint","text":"<p>The Load Checkpoint node can be used to load a diffusion model, diffusion models are used to denoise latents. This node will also provide the appropriate VAE and CLIP model.</p>"},{"location":"Core%20Nodes/Loaders/LoadCheckpoint/#inputs","title":"inputs","text":"<code>ckpt_name</code> <p>The name of the model.</p>"},{"location":"Core%20Nodes/Loaders/LoadCheckpoint/#outputs","title":"outputs","text":"<code>MODEL</code> <p>The model used for denoising latents.</p> <code>CLIP</code> <p>The CLIP model used for encoding text prompts.</p> <code>VAE</code> <p>The VAE model used for encoding and decoding images to and from latent space.</p>"},{"location":"Core%20Nodes/Loaders/LoadCheckpoint/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/LoadControlNet/","title":"Load ControlNet Model","text":"<p>The Load ControlNet Model node can be used to load a ControlNet model. Similar to how the CLIP model provides a way to give textual hints to guide a diffusion model, ControlNet models are used to give visual hints to a diffusion model. This process is different from e.g. giving a diffusion model a partially noised up image to modify. Instead ControlNet models can be used to tell the diffusion model e.g. where edges in the final image should be, or how subjects should be posed. This node can also be used to load T2IAdaptors.</p>"},{"location":"Core%20Nodes/Loaders/LoadControlNet/#inputs","title":"inputs","text":"<code>control_net_name</code> <p>The name of the ControlNet model.</p>"},{"location":"Core%20Nodes/Loaders/LoadControlNet/#outputs","title":"outputs","text":"<code>CONTROL_NET</code> <p>The ControlNet or T2IAdaptor model used for providing visual hints to a diffusion model.</p>"},{"location":"Core%20Nodes/Loaders/LoadControlNet/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/LoadLoRA/","title":"Load LoRA","text":"<p>The Load LoRA node can be used to load a LoRA. LoRAs are used to modify the diffusion and CLIP models, to alter the way in which latents are denoised. Typical use-cases include adding to the model the ability to generate in certain styles, or better generate certain subjects or actions. One can even chain multiple LoRAs together to further modify the model.</p> <p>Tip</p> <p>LoRA strength values can be set to negative values. At times this can result in interesting effects.</p>"},{"location":"Core%20Nodes/Loaders/LoadLoRA/#inputs","title":"inputs","text":"<code>model</code> <p>A diffusion model.</p> <code>clip</code> <p>A CLIP model.</p> <code>lora_name</code> <p>The name of the LoRA.</p> <code>strength_model</code> <p>How strongly to modify the diffusion model. This value can be negative.</p> <code>strength_clip</code> <p>How strongly to modify the CLIP model. This value can be negative.</p>"},{"location":"Core%20Nodes/Loaders/LoadLoRA/#outputs","title":"outputs","text":"<code>MODEL</code> <p>The modified diffusion model.</p> <code>CLIP</code> <p>The modified CLIP model.</p>"},{"location":"Core%20Nodes/Loaders/LoadLoRA/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/LoadStyleModel/","title":"Load Style Model","text":"<p>The Load Style Model node can be used to load a Style model. Style models can be used to provide a diffusion model a visual hint as to what kind of style the denoised latent should be in.</p> <p>Info</p> <p>Only T2IAdaptor style models are currently supported</p>"},{"location":"Core%20Nodes/Loaders/LoadStyleModel/#inputs","title":"inputs","text":"<code>style_model_name</code> <p>The name of the style model.</p>"},{"location":"Core%20Nodes/Loaders/LoadStyleModel/#outputs","title":"outputs","text":"<code>STYLE_MODEL</code> <p>The style model used for providing visual hints about the desired style to a diffusion model.</p>"},{"location":"Core%20Nodes/Loaders/LoadStyleModel/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/LoadUpscaleModel/","title":"Load Upscale Model","text":"<p>The Load Upscale Model node can be used to load a specific upscale model, upscale models are used to upscale images.</p>"},{"location":"Core%20Nodes/Loaders/LoadUpscaleModel/#inputs","title":"inputs","text":"<code>model_name</code> <p>The name of the upscale model.</p>"},{"location":"Core%20Nodes/Loaders/LoadUpscaleModel/#outputs","title":"outputs","text":"<code>UPSCALE_MODEL</code> <p>The upscale model used for upscaling images.</p>"},{"location":"Core%20Nodes/Loaders/LoadUpscaleModel/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Loaders/LoadVAE/","title":"Load VAE","text":"<p>The Load VAE node can be used to load a specific VAE model, VAE models are used to encoding and decoding images to and from latent space. Although the Load Checkpoint node provides a VAE model alongside the diffusion model, sometimes it can be useful to use a specific VAE model.</p>"},{"location":"Core%20Nodes/Loaders/LoadVAE/#inputs","title":"inputs","text":"<code>vae_name</code> <p>The name of the VAE.</p>"},{"location":"Core%20Nodes/Loaders/LoadVAE/#outputs","title":"outputs","text":"<code>VAE</code> <p>The VAE model used for encoding and decoding images to and from latent space.</p>"},{"location":"Core%20Nodes/Loaders/LoadVAE/#example","title":"example","text":"<p>At times you might wish to use a different VAE than the one that came loaded with the Load Checkpoint node. In the example below we use a different VAE to encode an image to latent space, and decode the result of the Ksampler.</p>"},{"location":"Core%20Nodes/Loaders/unCLIPCheckpointLoader/","title":"unCLIP Checkpoint Loader","text":"<p>The unCLIP Checkpoint Loader node can be used to load a diffusion model specifically made to work with unCLIP. unCLIP Diffusion models are used to denoise latents conditioned not only on the provided text prompt, but also on provided images. This node will also provide the appropriate VAE and CLIP amd CLIP vision models.</p> <p>Warning</p> <p>even though this node can be used to load all diffusion models, not all diffusion models are compatible with unCLIP.</p>"},{"location":"Core%20Nodes/Loaders/unCLIPCheckpointLoader/#inputs","title":"inputs","text":"<code>ckpt_name</code> <p>The name of the model.</p>"},{"location":"Core%20Nodes/Loaders/unCLIPCheckpointLoader/#outputs","title":"outputs","text":"<code>MODEL</code> <p>The model used for denoising latents.</p> <code>CLIP</code> <p>The CLIP model used for encoding text prompts.</p> <code>VAE</code> <p>The VAE model used for encoding and decoding images to and from latent space.</p> <code>CLIP_VISION</code> <p>The CLIP Vision model used for encoding image prompts.</p>"},{"location":"Core%20Nodes/Loaders/unCLIPCheckpointLoader/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Mask/","title":"Mask","text":"<p>Masks provide a way to tell the sampler what to denoise and what to leave alone. These nodes provide a variety of ways create or load masks and manipulate them.</p>"},{"location":"Core%20Nodes/Mask/ConvertImageToMask/","title":"Convert Image to Mask","text":"<p>The Convert Image yo Mask node can be used to convert a specific channel of an image into a mask.</p>"},{"location":"Core%20Nodes/Mask/ConvertImageToMask/#inputs","title":"inputs","text":"<code>image</code> <p>The pixel image to be converted to a mask.</p> <code>channel</code> <p>Which channel to use as a mask.</p>"},{"location":"Core%20Nodes/Mask/ConvertImageToMask/#outputs","title":"outputs","text":"<code>MASK</code> <p>The mask created from the image channel.</p>"},{"location":"Core%20Nodes/Mask/ConvertImageToMask/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Mask/ConvertMaskToImage/","title":"Convert Mask to Image","text":"<p>The Convert Mask to Image node can be used to convert a mask to a grey scale image.</p>"},{"location":"Core%20Nodes/Mask/ConvertMaskToImage/#inputs","title":"inputs","text":"<code>mask</code> <p>The mask to be converted to an image.</p>"},{"location":"Core%20Nodes/Mask/ConvertMaskToImage/#outputs","title":"outputs","text":"<code>IMAGE</code> <p>The grey scale image from the mask.</p>"},{"location":"Core%20Nodes/Mask/ConvertMaskToImage/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Mask/CropMask/","title":"Crop Mask","text":"<p>The Crop Mask node can be used to crop a mask to a new shape.</p> <p>Info</p> <p>The origin of the coordinate system in ComfyUI is at the top left corner.</p>"},{"location":"Core%20Nodes/Mask/CropMask/#inputs","title":"inputs","text":"<code>mask</code> <p>The mask to be cropped.</p> <code>width</code> <p>The width of the area in pixels.</p> <code>height</code> <p>The height of the area in pixels.</p> <code>x</code> <p>The x coordinate of the area in pixels.</p> <code>y</code> <p>The y coordinate of the area in pixels.</p>"},{"location":"Core%20Nodes/Mask/CropMask/#outputs","title":"outputs","text":"<code>MASK</code> <p>The cropped mask.</p>"},{"location":"Core%20Nodes/Mask/CropMask/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Mask/FeatherMask/","title":"Feather Mask","text":"<p>The Feather Mask node can be used to feather a mask.</p>"},{"location":"Core%20Nodes/Mask/FeatherMask/#inputs","title":"inputs","text":"<code>mask</code> <p>The mask to be feathered.</p> <code>left</code> <p>How much to feather edges on the left</p> <code>top</code> <p>How much to feather edges on the top</p> <code>right</code> <p>How much to feather edges on the right</p> <code>bottom</code> <p>How much to feather edges on the bottom</p>"},{"location":"Core%20Nodes/Mask/FeatherMask/#outputs","title":"outputs","text":"<code>MASK</code> <p>The feathered mask.</p>"},{"location":"Core%20Nodes/Mask/FeatherMask/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Mask/InvertMask/","title":"Invert Mask","text":"<p>The Invert Mask node can be used to invert a mask.</p>"},{"location":"Core%20Nodes/Mask/InvertMask/#inputs","title":"inputs","text":"<code>mask</code> <p>The mask to be inverted.</p>"},{"location":"Core%20Nodes/Mask/InvertMask/#outputs","title":"outputs","text":"<code>MASK</code> <p>The inverted mask.</p>"},{"location":"Core%20Nodes/Mask/InvertMask/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Mask/LoadImageAsMask/","title":"Load Image (as Mask)","text":"<p>The Load Image (as Mask) node can be used to load a channel of an image to use as a mask. Images can be uploaded by starting the file dialog or by dropping an image onto the node. Once the image has been uploaded they can be selected inside the node.</p> <p>Info</p> <p>by default images will be uploaded to the input folder of ComfyUI</p>"},{"location":"Core%20Nodes/Mask/LoadImageAsMask/#inputs","title":"inputs","text":"<code>image</code> <p>The name of the image to be converted to a mask.</p> <code>channel</code> <p>The channel of the image to be used as mask.</p>"},{"location":"Core%20Nodes/Mask/LoadImageAsMask/#outputs","title":"outputs","text":"<code>MASK</code> <p>The mask created from the image channel.</p>"},{"location":"Core%20Nodes/Mask/LoadImageAsMask/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Mask/MaskComposite/","title":"Mask Composite","text":"<p>The Mask Composite node can be used to paste one mask into another.</p> <p>Info</p> <p>The origin of the coordinate system in ComfyUI is at the top left corner.</p>"},{"location":"Core%20Nodes/Mask/MaskComposite/#inputs","title":"inputs","text":"<code>destination</code> <p>The mask that is to be pasted in.</p> <code>source</code> <p>The mask that is to be pasted.</p> <code>x</code> <p>The x coordinate of the pasted mask in pixels.</p> <code>y</code> <p>The y coordinate of the pasted mask in pixels.</p> <code>operation</code> <p>how to paste the mask.</p>"},{"location":"Core%20Nodes/Mask/MaskComposite/#outputs","title":"outputs","text":"<code>MASK</code> <p>A new mask composite containing the <code>source</code> pasted into <code>destination</code>.</p>"},{"location":"Core%20Nodes/Mask/MaskComposite/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Mask/SolidMask/","title":"Solid Mask","text":"<p>The Solid Mask node can be used to create a solid masking containing a single value.</p>"},{"location":"Core%20Nodes/Mask/SolidMask/#inputs","title":"inputs","text":"<code>value</code> <p>The value to fill the mask with.</p> <code>width</code> <p>The width of the mask.</p> <code>height</code> <p>The height of the mask.</p>"},{"location":"Core%20Nodes/Mask/SolidMask/#outputs","title":"outputs","text":"<code>MASK</code> <p>The mask filled with a single value.</p>"},{"location":"Core%20Nodes/Mask/SolidMask/#example","title":"example","text":"<p>example usage text with workflow image</p>"},{"location":"Core%20Nodes/Sampling/","title":"Sampling","text":"<p>The sampling nodes provide a way to denoise latent images using a diffusion model. For an overview of the available schedules and samplers, see here</p>"},{"location":"Core%20Nodes/Sampling/KSampler%20Advanced/","title":"KSampler Advanced","text":"<p>The KSampler Advanced node is the more advanced version of the KSampler node. While the KSampler node always adds noise to the latent followed by completely denoising the noised up latent, the KSampler Advanced node provides extra settings to control this behavior. The KSampler Advanced node can be told not to add noise into the latent with the <code>add_noise</code> setting. It can also be made to return partially denoised images via the <code>return_with_leftover_noise</code> setting. Unlike the KSampler node, this node does not have a <code>denoise</code> setting but this process is instead controlled by the <code>start_at_step</code> and <code>end_at_step</code> settings. This makes it possible to e.g. hand over a partially denoised latent to a separate KSampler Advanced node to finish the process.</p> <p>Tip</p> <p>Given that <code>end_at_step &gt;= steps</code> a KSampler Advanced node will denoise a latent in the exact same way a KSampler node would with a <code>denoise</code> setting of:</p> <p><code>denoise = (steps - start_at_step) / steps</code></p>"},{"location":"Core%20Nodes/Sampling/KSampler%20Advanced/#inputs","title":"inputs","text":"<code>Model</code> <p>The model used for denoising</p> <code>Positive</code> <p>The positive conditioning.</p> <code>Negative</code> <p>The negative conditioning.</p> <code>latent_image</code> <p>The latent that will be denoised.</p> <code>add_noise</code> <p>Wether or not to add noise to the latent before denoising. When enabled the node will inject noise appropriate for the given start step.</p> <code>seed</code> <p>The random seed used in creating the noise.</p> <code>control_after_generate</code> <p>Provides the ability to change the seed number described above after each prompt. the node can <code>randomize</code>, <code>increment</code>, <code>decrement</code> or keep the seed number <code>fixed</code>.</p> <code>steps</code> <p>The number of steps in the schedule. The more steps the sampler is allowed to make the more accurate the result will be. See the samplers page for good guidelines on how to pick an appropriate number of steps.</p> <code>cfg</code> <p>The classifier free guidance(cfg) scale determines how aggressive the sampler should be in realizing the content of the prompts in the final image. Higher scales force the image to better represent the prompt, but a scale that is set too high will negatively impact the quality of the image.</p> <code>sampler_name</code> <p>Which sampler to use, see the samplers page for more details on the available samplers.</p> <code>scheduler</code> <p>The type of schedule to use, see the samplers page for more details on the available schedules.</p> <code>start_at_step</code> <p>Determines at which step of the schedule to start the denoising process.</p> <code>end_at_step</code> <p>Determines at which step of the schedule to end denoising. When this settings exceeds <code>steps</code> the schedule ends at <code>steps</code> instead</p> <code>return_with_leftover_noise</code> <p>When disabled the KSampler Advanced will attempt to completely denoise the latent in the final step. Depending on how many steps in the schedule are skipped by this, the output can be inaccurate and of lower quality.</p>"},{"location":"Core%20Nodes/Sampling/KSampler%20Advanced/#outputs","title":"outputs","text":"<code>LATENT</code> <p>the denoised latent.</p>"},{"location":"Core%20Nodes/Sampling/KSampler%20Advanced/#example","title":"example","text":"<p>Under construction</p>"},{"location":"Core%20Nodes/Sampling/KSampler/","title":"KSampler","text":"<p>The KSampler uses the provided model and positive and negative conditioning to generate a new version of the given latent. First the latent is noised up according to the given <code>seed</code> and <code>denoise</code> strength, erasing some of the latent image. then this noise is removed using the given <code>Model</code> and the <code>positive</code> and <code>negative</code> conditioning as guidance, \"dreaming\" up new details in places where the image was erased by noise.</p>"},{"location":"Core%20Nodes/Sampling/KSampler/#inputs","title":"inputs","text":"<code>Model</code> <p>The model used for denoising</p> <code>Positive</code> <p>The positive conditioning.</p> <code>Negative</code> <p>The negative conditioning.</p> <code>latent_image</code> <p>The latent that will be denoised.</p> <code>seed</code> <p>The random seed used in creating the noise.</p> <code>control_after_generate</code> <p>Provides the ability to change the seed number described above after each prompt. the node can <code>randomize</code>, <code>increment</code>, <code>decrement</code> or keep the seed number <code>fixed</code>.</p> <code>steps</code> <p>The number of steps to use during denoising. The more steps the sampler is allowed to make the more accurate the result will be. See the samplers page for good guidelines on how to pick an appropriate number of steps.</p> <code>cfg</code> <p>The classifier free guidance(cfg) scale determines how aggressive the sampler should be in realizing the content of the prompts in the final image. Higher scales force the image to better represent the prompt, but a scale that is set too high will negatively impact the quality of the image.</p> <code>sampler_name</code> <p>Which sampler to use, see the samplers page for more details on the available samplers.</p> <code>scheduler</code> <p>The type of schedule to use, see the samplers page for more details on the available schedules.</p> <code>denoise</code> <p>How much information of the latents should be erased by noise.</p>"},{"location":"Core%20Nodes/Sampling/KSampler/#outputs","title":"outputs","text":"<code>LATENT</code> <p>the denoised latent.</p>"},{"location":"Core%20Nodes/Sampling/KSampler/#example","title":"example","text":"<p>The KSampler is the core of any workflow and can be used to perform text to image and image to image generation tasks. The example below shows how to use the KSampler in an image to image task, by connecting a model, a positive and negative embedding, and a latent image. Note that we use a denoise value of less than 1.0. This way parts of the original image are preserved when it is noised up, guiding the denoising process to similar looking images.</p>"},{"location":"Core%20Nodes/Sampling/samplers/","title":"Samplers","text":"<p>under construction</p>"},{"location":"Custom%20Nodes/","title":"Overview page of developing ComfyUI custom nodes stuff","text":""},{"location":"Developing%20Custom%20Nodes/","title":"Overview page of ComfyUI interface stuff","text":""},{"location":"Interface/","title":"Overview page of ComfyUI interface stuff","text":"<p>under construction</p>"},{"location":"Interface/SaveFileFormatting/","title":"Save File Formatting","text":"<p>It can be hard to keep track of all the images that you generate. To help with organizing your images you can pass specially formatted strings to an output node with a <code>file_prefix</code> widget.</p>"},{"location":"Interface/SaveFileFormatting/#search-and-replace-strings","title":"Search and replace strings","text":"<p>To automatically insert the values of certain node widgets into the file name the following syntax can be used: <code>%node_name.widget_name%</code> e.g. if we wish to store images on a per resolution bases we could provide the node with the following string: <code>%Empty Latent Image.width%x%Empty Latent Image.height%/image</code>. These string will then be replaced by the specified node values.</p>"},{"location":"Interface/SaveFileFormatting/#renaming-nodes-for-search-and-replace","title":"Renaming nodes for search and replace","text":"<p>At times node names might be rather large or multiple nodes might share the same name. In these cases one can specify a specific name in the node option menu under <code>properties&gt;Node name for S&amp;R</code></p> <p></p>"},{"location":"Interface/SaveFileFormatting/#date-time-strings","title":"Date time strings","text":"<p>ComfyUI can also inset date information with <code>%date:FORMAT%</code> where format recognizes the following specifiers:</p> specifier description <code>d</code> or <code>dd</code> day <code>M</code> or <code>MM</code> month <code>yy</code> or <code>yyyy</code> year <code>h</code> or <code>hh</code> hour <code>m</code> or <code>mm</code> minute <code>s</code> or <code>ss</code> second"},{"location":"Interface/Shortcuts/","title":"Shortcuts","text":"<p>ComfyUI comes with the following shortcuts you can use to speed up your workflow:</p> Keybind Explanation Ctrl+Enter Queue up current graph for generation Ctrl+Shift+Enter Queue up current graph as first for generation Ctrl+S Save workflow Ctrl+O Load workflow Ctrl+A Select all nodes Ctrl+M Mute/unmute selected nodes Del Delete selected nodes Backspace Delete selected nodes Ctrl+Del Delete the current graph Ctrl+Backspace Delete the current graph Space Move the canvas around when held and moving the cursor Ctrl+Left Button Add clicked node to selection Shift+Left Button Add clicked node to selection Ctrl+C Copy selected nodes Ctrl+V Paste selected nodes while severing connections Ctrl+Shift+V Paste selected nodes while maintaining incoming connections Shift+Left Button Hold and drag to move multiple selected nodes at the same time Ctrl+D Load default graph Q Toggle visibility of the queue H Toggle visibility of history R Refresh graph 2 X Left Button Double click to open node quick search palette Right Button Open node menu"},{"location":"Interface/Textprompts/","title":"Text Prompts","text":"<p>ComfyUI Provides a variety of ways to finetune your prompts to better reflect your intention.</p>"},{"location":"Interface/Textprompts/#up-and-down-weighting","title":"up and down weighting","text":"<p>The importance of parts of the prompt can be up or down-weighted by enclosing the specified part of the prompt in brackets using the following syntax: <code>(prompt:weight)</code>. E.g. if we have a prompt <code>flowers inside a blue vase</code> and we want the diffusion model to empathize the flowers we could try reformulating our prompt into: <code>(flowers:1.2) inside a blue vase</code>. Nested loops multiply the weights inside them, e.g. in the prompt <code>((flowers:1.2):.5) inside a blue vase</code> flowers end up with a weight of 0.6. Using only brackets without specifying a weight is shorthand for <code>(prompt:1.1)</code>, e.g. <code>(flower)</code> is equal to <code>(flower:1.1)</code>. To use brackets inside a prompt they have to be escaped, e.g. <code>\\(1990\\)</code>. ComfyUI can also add the appropriate weighting syntax for a selected part of the prompt via the keybinds Ctrl+Up and Ctrl+Down. The amount by which these shortcuts up or down-weight can be adjusted in the settings.</p>"},{"location":"Interface/Textprompts/#using-textual-inversion-embeddings","title":"using textual inversion embeddings","text":"<p>Textual inversions are custom made CLIP embeddings that embody certain concepts. Textual inversions can be referenced inside a prompt by use the following syntax: <code>embedding:name</code> where name is the name of the embedding file.</p>"},{"location":"Interface/Textprompts/#adding-random-choices","title":"adding random choices","text":"<p>It is possible to let ComfyUI choose random parts of a prompt when it is queued up using the following syntax <code>{choice1|choice2|...}</code>. E.g. if we want ComfyUI to randomly select one of a set of colors we can add the following to our prompt: <code>{red|blue|yellow|green}</code>.</p>"},{"location":"Interface/UtilityNodes/","title":"Utility Nodes","text":"<p>ComfyUI comes with a set of nodes to help manage the graph.</p>"},{"location":"Interface/UtilityNodes/#reroute","title":"Reroute","text":"<p>The Reroute node can be used to reroute links, this can be useful for organizing your workflows.</p> <p>Tip</p> <p>the in and output on the reroute node can also be positioned vertically</p>"},{"location":"Interface/UtilityNodes/#primitive","title":"Primitive","text":"<p>The Primitive node can be used to...</p>"}]}